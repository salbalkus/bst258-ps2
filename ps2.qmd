---
title: "Problem Set #2"
subtitle: "BST 258: Causal Inference -- Theory and Practice"
author: "Salvador Balkus"
date: ""
format:
  pdf:
    documentclass: scrartcl
    papersize: letter
    fontsize: 11pt
    geometry:
      - margin=1in
      - heightrounded
    number-sections: false
    colorlinks: true
    link-citations: true
    callout-appearance: simple
    callout-icon: false
    # figure options
    fig-width: 6
    fig-asp: 0.618
    fig-cap-location: bottom
    # code block options
    code-line-numbers: false
    code-block-bg: false
    highlight-style: nord
bibliography: refs.bib
---

```{r}
#| echo: false
#| message: false
#| label: global-setup
# NOTE: The immediately following line loads an renv environment located at the
#       nearest "top-level" directory, as marked by a `.here` file, which is
#       located by the here::here() function. This would be a useful tool if,
#       say, this template.qmd file was not located at the top-level directory.
#       Here, renv should activate automatically when this file is opened.
#renv::load(here::here())
library(here)
library(tidyverse)
library(geepack)
```

## Question 1: Inverse Probability Weighting

### Part 1: Theory

#### 1) 

#### 2)

The goal is to show, under conditional exchangeability, positivity, and consistency, that the IPW mean matches the counterfactual mean. 

\begin{align*}
E\Big(\frac{I(A = a)Y}{P(A|L)}\Big) &= E\Big(E\Big(\frac{I(A = a)}{P(A|L)}Y|L\Big)\Big) & \text{(iterated expectation law)} \\
&= E\Big(E\Big(\frac{1}{P(A|L)}Y^a|L\Big)\Big) & \text{(consistency)}\\
&= E\Big(E\Big(\frac{1}{P(A|L)}|L)\cdot E\Big(Y^a|L\Big)\Big) & \text{(conditional exchangeability)}\\
&= E\Big(\int\frac{1}{P(A|L)}\cdot P(A|L)da\cdot E\Big(Y^a|L\Big)\Big) & \text{(positivity)}\\
&= E(E(Y^a|L)) = E(Y^a) & \text{(iterated expectation law)}
\end{align*}

### Part 2: Application - Inverse Probability Weighted Estimation

```{r}
library(fastverse)
library(readxl)
library(stringr)
# create URLs for downloading NHEFS data
url_trunks <- c("2012/10/nhefs_sas.zip", "2012/10/nhefs_stata.zip",
"2017/01/nhefs_excel.zip", "1268/20/nhefs.csv")
url_stub <- "https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/"
data_urls <- lapply(url_trunks, function(url_trunk) {
paste0(url_stub, url_trunk)
})
# download and unzip files
temp <- tempfile()
for (i in seq_len(sum(str_count(url_trunks, "zip")))) {
download.file(data_urls[[i]], temp)
unzip(temp, exdir = "data")
}
download.file(data_urls[[4]], "data/nhefs.csv")
```

#### a)

The code below generates IP weights and stabilized weights.

```{r}
df <- read.csv(here("data", "nhefs.csv"))
df <- df %>% 
  select(wt82_71, qsmk, sex, age, race, education, smokeintensity, smokeyrs, active, exercise, wt71) %>% 
  transform(education = factor(education), active = factor(active), exercise = factor(exercise))


prop_model <- glm(qsmk ~ sex + age + race + education + smokeintensity + smokeyrs + active + exercise + wt71 + age^2 + wt71^2 + smokeintensity^2 + smokeyrs^2, data = df, family = binomial(link='logit'))
prop <- predict(prop_model, df, type="response")
p <- mean(df$qsmk)

`IPW Weights` <- ifelse(df$qsmk, 1 / prop, 1 / (1-prop))
`Stabilized IPW Weights` <- `IPW Weights` * ifelse(df$qsmk, p, 1-p)

```

```{r}
par(mfrow = c(1,2))
hist(`IPW Weights`)
hist(`Stabilized IPW Weights`)
```

As we can see by comparing the histograms, although both sets of weights follow the same distribution, the stabilized weights are far less skewed, and contain fewer large values.

#### b)

To calculate the ATE with sandwich standard errors, we can use fit a GEE model with independent correlation structure, weighted by the corresponding IPW weights. The code below does this and displays 95% Wald-style CIs

```{r}

# Compute ATE
df$id <- 1:nrow(df)
ate_ipw_fit <- geeglm(wt82_71 ~ qsmk, weights = `IPW Weights`, data = df, id = id, corstr = "independence")
ate_sipw_fit <- geeglm(wt82_71 ~ qsmk, weights = `Stabilized IPW Weights`, data = df, id = id, corstr = "independence")

getci <- function(fit, name){
  c <- coef(summary(fit))["qsmk",]
  ci <- with(as.data.frame(c),
            cbind(
                  ATE = Estimate,
                  Std.Err = Std.err,
                  Lower = Estimate-1.96*Std.err,
                  Upper = Estimate+1.96*Std.err
                  ))
  rownames(ci) <- name
  return(ci)
}

ipw <- rbind(
  getci(ate_sipw_fit, "ipw"),
  getci(ate_sipw_fit, "sipw")
)

ipw


```

#### c)

Two methods to estimate the variance of the ATE include the sandwich variance estimator (from estimating equation theory) and bootstrapping. See the final output of part (b) above for computed Wald-style confidence intervals based on the sandwich standard error using GEE.

#### d)

The stabilized and non-stabilized weights yield exactly the same estimates. This is because in this dataset, there are no extreme propensity weights observed, so there are no instability issues for stabilization to "fix." This likely occurred because most of the  covariates for which we adjusted were mostly simple categorical features, and for these types of covariates it is difficult to get extremely low propensity scores or near-positivity-violations with such a large dataset.


### Part 2: Application - Doubly Robust Estimation

#### a)

Below, we fit a linear outcome regression model and extract outcome predictions for the doubly-robust estimator.

```{r}

# Fit the outcome regression model
mAL_fit <- lm(wt82_71 ~ qsmk + qsmk*smokeintensity + sex + age + age^2 + 
                race + education + smokeintensity + smokeintensity^2 + 
                smokeyrs + smokeyrs^2 + active + exercise + 
                wt71 + wt71^2, data = df)

# Duplicate data to get counterfactual predictions
A0 = df
A0$qsmk = 0
A1 = df
A1$qsmk = 1

mAL <- predict(mAL_fit, df, type = "response")
m0L <- predict(mAL_fit, A0, type = "response")
m1L <- predict(mAL_fit, A1, type = "response")

```

#### b)

The code below computes the DR estimator using the previous predictions and IPW weights.

```{r}
A <- df$qsmk
Y <- df$wt82_71

# Note that the 1 - g(L) weights are already included in the ipw_c_weight variable
D <- na.omit((A*`IPW Weights` - (1 - A)*`IPW Weights`) * (Y - mAL) + (m1L - m0L))
ATE_DR <- mean(D)

cat("Doubly-Robust Estimate of ATE: ", ATE_DR, "\n")
cat("IPW Weighted Estimate of ATE: ", ipw[1,1], "\n")
cat("SIPW Weighted Estimate of ATE: ", ipw[2,1])

```

We can see that the ATE doubly-robust estimate is very similar to IPW, just slightly smaller This is logical, since both the DR and IPW estimators should be consistent if the parametric models are correct. Since we use the same parametric models for both with a reasonably large sample, it makes sense that both point estimates are similar in magnitude.

#### c)

The code below compute the standard error of the DR estimator, and places it side-by-side with the IPW standard errors.

```{r}
SE_DR = sqrt(var(D) / length(D))
c(dr = SE_DR, ipw[,"Std.Err"])
```

As we can see, the doubly-robust estimator has a slightly lower variance. This is expected, since one of the chief benefits of the doubly-robust estimator is its superior efficiency. In fact, if both parametric models are correctly specified, the doubly-robust estimator variance achieves the semiparametric efficiency bound; it is the lowest possible variance among all regular and asymptotically normal estimators. Since both the IPW and DR estimators rely on the same parametric model to estimate IP weights, it makes sense that DR would be more efficient than IPW.

{{< pagebreak >}}


## Question 2: Standardization and Parametric G-Computation

### Part 1: Theory

#### 1)

Let us show that the standardized mean and the IPW mean are equivalent; that is,

$$\sum_lE(Y|A=a, L=l)\cdot P(L=l) = E\Big(\frac{I(A=a)}{P(A|L)}Y\Big)$$

Proceed as follows:

\begin{align*}
\sum_lE(Y|A=a, L=l)\cdot P(L=l) &= \sum_l\int Y\cdot P(Y=y|A=a, L=l) dy \cdot P(L = l)\\
&= \sum_l\int \frac{P(A=a|L=l)}{P(A=a|L=l)}Y\cdot P(Y=y|A=a, L=l) dy \cdot P(L = l)\\
&= \sum_l\int \frac{P(A=a, L=l)}{P(L = l) \cdot P(A=a|L=l)}\cdot P(L = l) \cdot Y\cdot P(Y=y|A=a, L=l)dy\\
&= \sum_l\int \frac{1}{P(A=a|L=l)}Y\cdot P(Y=y, A=a, L=l)dy\\
\end{align*}

since $P(Y=y, A=a, L=l) = P(Y=y|A=a, L=l) \cdot P(A= a, L = l)$ by definition of conditional probability. Furthermore, by the definition of multivariate integration, we have that the above is equal to 

\begin{align*}
\sum_l\int \frac{1}{P(A=a|L=l)}Y\cdot P(Y=y, A=a, L=l)dy\\
= E\Big(E_L\Big(\frac{I(A = a, L = l)}{P(A|L)}Y\Big)\Big)\\
= E\Big(\frac{I(A = a)}{P(A|L)}Y\Big)
\end{align*}

which completes the proof that the standardized mean and IPW mean are equivalent.

#### 2)

Under conditional exchangeability and consistency, we can show that the standardized mean is equal to the mean of the potential outcome as follows:

\begin{align*}
\sum_lE(Y|A=a, L=l)\cdot P(L=l) &= \sum_lE(Y^a|A=a, L=l)\cdot P(L=l) & \text{(consistency)}\\
&= \sum_lE(Y^a| L=l)\cdot P(L=l)  & \text{(conditional exchangeability)}\\
&= E(E(Y^a | L = l)) & \text{(definition of expectation)}\\
&= E(Y^a) & \text{(iterated expectation law)}
\end{align*}

#### 3)

If we can be sure that the outcome model is correct, we would prefer the plug-in estimator if we do not know the form of $\hat{g}(L)$ and/or it is hard to estimate. This can occur, for example, if $L$ is high-dimensional with lots of continuous covariates. In fact, for certain parametric outcome models, the plug-in will also be doubly-robust and will achieve comparable variance, so there is no advantage to using a DR method.

However, if $\hat{m}_A$ is nonparametric, and $\hat{g}(L)$ can be estimated, then we would prefer the doubly-robust estimator because it will probably achieve lower variance. If we use a nonparametric model for the plug-in, we've seen that the plug-in is asymptotically biased; estimating $\hat{g}(L)$ and using the doubly-robust method will reduce variance (not to mention still be consistent if $\hat{m}_A$ actually winds up being misspecified).

### Part 2: Application (1)

#### a)

Since we already fit a linear model of the requested nature in the previous question, and obtained predictions for both counterfactual outcomes, we will reuse it and apply g-computation using the predictions we obtained earlier:

```{r}

ATE_GC <- mean(m1L) - mean(m0L)
cat("G-Computation Estimate of ATE: ", ATE_GC)

```

{{< pagebreak >}}

#### b)

This is similar to the IPW estimate of the ATE, which was `r ipw[1,1]`, but not exactly the same. The difference likely results from the the fact that the models we've used here rely on different parametric assumptions. IPW assumes a logistic model for the propensity, while standardization assumes a linear model for the conditional mean.

#### c)

Although IPW and standardization means are equivalent, they might not match exactly since different models are used to estimate their nuisance parameters. For example, in the parametric setting of this question, IPW relies on logistic regression, while standardization relies on linear regression. Since these have subtly different parametric assumptions (IPW that the propensities follow a logit-linear model, standardization that the conditional mean is linear), the differences in these assumptions may lead to subtly different estimates of the ATE. More generally, one generally wouldn't use the same modeling technique to estimate a propensity score and a conditional mean (unless $Y$ is also binary); therefore, standardization and IPW estimates would generally differ based on which models are used to estimate nuisances. 

### Part 2: Application (2)

#### a) 

"Doubly-robust" refers to the fact that the estimator will be consistent if either the outcome regression $\hat{m}_A(L)$ is consistent OR if the propensity score $\hat{g}(L)$ is consistent. Even if one nuisance model is wrong (but still converges to *something*, even if it is incorrect), then the DR estimator will still be consistent, though it will be inefficient.

#### b)

Since we've already implemented the doubly-robust estimator in Question 1, Part 2: Doubly-Robust Estimation and computed an estimate of the ATE with standard errors, we can compute a 95% confidence interval and report it along with point estimates and standard error below:


```{r}
c(estimate = ATE_DR, SE = SE_DR, Lower = ATE_DR - 1.96 * SE_DR, Upper = ATE_DR + 1.96 * SE_DR)
```
Note that 0 is well outside the bounds of this effect estimate, so clearly the average treatment effect of smoking cessation on weight gain is statistically significant at the $\alpha = 0.05$ level. 


## References

::: {#refs}
:::

